{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MusicGeneration.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1JdOCyx5XN13d2GrvdTQaBsW4QajcqRsu","authorship_tag":"ABX9TyPjtWwj4SP7pY1yUh5eebbY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"XhiSQ343wGfN"},"source":["!git clone https://github.com/chathasphere/pno-ai.git "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n4ec2m9rwNvC"},"source":["!wget https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I0c9cLZnwO_Q"},"source":["!unzip -q maestro-v2.0.0-midi.zip -d data/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-N8QWgDwiY8"},"source":["!pip install pretty_midi "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9p_GTv6t1Etf"},"source":["!mv data/maestro-v2.0.0/2004 pno-ai/data/maestro-v2.0.0 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KcbG2IjRkayF"},"source":["cd pno-ai/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A9qBUqKPweVt"},"source":["import os, time, datetime\n","import torch \n","import torch.nn as nn\n","from random import shuffle\n","from preprocess import PreprocessingPipeline\n","from train import train\n","import argparse\n","import numpy as np\n","import torch.nn.functional as F\n","from preprocess import SequenceEncoder "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q5bWhKS3kmgo"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from helpers import clones, d\n","import math \n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XWpy5HlUk4oU"},"source":["class AttentionError(Exception):\n","    pass\n","\n","class MultiheadedAttention(nn.Module):\n","    \"\"\"\n","    Narrow multiheaded attention. Each attention head inspects a \n","    fraction of the embedding space and expresses attention vectors for each sequence position as a weighted average of all (earlier) positions.\n","    \"\"\"\n","\n","    def __init__(self, d_model, heads=8, dropout=0.1, relative_pos=True):\n","\n","        super().__init__()\n","        if d_model % heads != 0:\n","            raise AttentionError(\"Number of heads does not divide model dimension\")\n","        self.d_model = d_model\n","        self.heads = heads\n","        s = d_model // heads\n","        self.linears = torch.nn.ModuleList([nn.Linear(s, s, bias=False) for i in range(3)])\n","        self.recombine_heads = nn.Linear(heads * s, d_model)\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.max_length = 256\n","        #relative positional embeddings\n","        self.relative_pos = relative_pos\n","        if relative_pos:\n","            self.Er = torch.randn([heads, self.max_length, s],\n","                    device=d())\n","        else:\n","            self.Er = None\n","\n","    def forward(self, x, mask):\n","        #batch size, sequence length, embedding dimension\n","        b, t, e = x.size()\n","        h = self.heads\n","        #each head inspects a fraction of the embedded space\n","        #head dimension\n","        s = e // h\n","        #start index of position embedding\n","        embedding_start = self.max_length - t\n","        x = x.view(b,t,h,s)\n","        queries, keys, values = [w(x).transpose(1,2)\n","                for w, x in zip(self.linears, (x,x,x))]\n","        if self.relative_pos:\n","            #apply same position embeddings across the batch\n","            #Is it possible to apply positional self-attention over\n","            #only half of all relative distances?\n","            Er  = self.Er[:, embedding_start:, :].unsqueeze(0)\n","            QEr = torch.matmul(queries, Er.transpose(-1,-2))\n","            QEr = self._mask_positions(QEr)\n","            #Get relative position attention scores\n","            #combine batch with head dimension\n","            SRel = self._skew(QEr).contiguous().view(b*h, t, t)\n","        else:\n","            SRel = torch.zeros([b*h, t, t], device=d())\n","        queries, keys, values = map(lambda x: x.contiguous()\\\n","                .view(b*h, t, s), (queries, keys, values))\n","        #Compute scaled dot-product self-attention\n","        #scale pre-matrix multiplication   \n","        queries = queries / (e ** (1/4))\n","        keys    = keys / (e ** (1/4))\n","\n","        scores = torch.bmm(queries, keys.transpose(1, 2))\n","        scores = scores + SRel\n","        #(b*h, t, t)\n","\n","        subsequent_mask = torch.triu(torch.ones(1, t, t, device=d()),\n","                1)\n","        scores = scores.masked_fill(subsequent_mask == 1, -1e9)\n","        if mask is not None:\n","            mask = mask.repeat_interleave(h, 0)\n","            wtf = (mask == 0).nonzero().transpose(0,1)\n","            scores[wtf[0], wtf[1], :] = -1e9\n","\n","        \n","        #Convert scores to probabilities\n","        attn_probs = F.softmax(scores, dim=2)\n","        attn_probs = self.dropout(attn_probs)\n","        #use attention to get a weighted average of values\n","        out = torch.bmm(attn_probs, values).view(b, h, t, s)\n","        #transpose and recombine attention heads\n","        out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n","        #last linear layer of weights\n","        return self.recombine_heads(out)\n","\n","\n","    def _mask_positions(self, qe):\n","        #QEr is a matrix of queries (absolute position) dot distance embeddings (relative pos).\n","        #Mask out invalid relative positions: e.g. if sequence length is L, the query at\n","        #L-1 can only attend to distance r = 0 (no looking backward).\n","        L = qe.shape[-1]\n","        mask = torch.triu(torch.ones(L, L, device=d()), 1).flip(1)\n","        return qe.masked_fill((mask == 1), 0)\n","\n","    def _skew(self, qe):\n","        #pad a column of zeros on the left\n","        padded_qe = F.pad(qe, [1,0])\n","        s = padded_qe.shape\n","        padded_qe = padded_qe.view(s[0], s[1], s[3], s[2])\n","        #take out first (padded) row\n","        return padded_qe[:,:,1:,:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2iHnW-ZDk4Zt"},"source":["class MusicTransformerError(Exception):\n","    pass\n","\n","class MusicTransformer(nn.Module):\n","    \"\"\"Generative, autoregressive transformer model. Train on a \n","    dataset of encoded musical sequences.\"\"\"\n","\n","    def __init__(self, n_tokens, seq_length=None, d_model=64,\n","            n_heads=4, depth=2, d_feedforward=512, dropout=0.1,\n","            positional_encoding=False, relative_pos=True, gen=False):\n","        \"\"\"\n","        Args:\n","            n_tokens: number of commands/states in encoded musical sequence\n","            seq_length: length of (padded) input/target sequences\n","            d_model: dimensionality of embedded sequences \n","            n_heads: number of attention heads\n","            depth: number of stacked transformer layers\n","            d_feedforward: dimensionality of dense sublayer \n","            dropout: probability of dropout in dropout sublayer\n","            relative_pos: (bool) if True, use relative positional embeddings\n","        \"\"\"\n","        super().__init__()\n","        self.gen = gen\n","        #number of commands in an encoded musical sequence\n","        self.n_tokens = n_tokens\n","        #embedding layer\n","        self.d_model = d_model\n","        self.embed = SequenceEmbedding(n_tokens, d_model)\n","        #positional encoding layer\n","        self.positional_encoding = positional_encoding\n","        if self.positional_encoding:\n","            pos = torch.zeros(5000, d_model)\n","            position = torch.arange(5000).unsqueeze(1)\n","            #geometric progression of wave lengths\n","            div_term = torch.exp(torch.arange(0.0, d_model, 2) * \\\n","                            - (math.log(10000.0) / d_model))\n","\t    #even positions\n","            pos[0:, 0::2] = torch.sin(position * div_term)\n","            #odd positions\n","            pos[0:, 1::2] = torch.cos(position * div_term)\n","            #batch dimension\n","            pos = pos.unsqueeze(0)\n","            #move to GPU if needed\n","            pos = pos.to(d())\n","            self.register_buffer('pos', pos)\n","        else:\n","            if seq_length == None:\n","                raise MusicTransformerError(\"seq_length not provided for positional embeddings\")\n","            self.pos = nn.Embedding(seq_length, d_model)\n","        #last layer, outputs logits of next token in sequence\n","        self.to_scores = nn.Linear(d_model, n_tokens)\n","        self.layers = clones(DecoderLayer(d_model, n_heads,\n","            d_feedforward, dropout, relative_pos), depth)\n","        self.norm = nn.LayerNorm(d_model)\n","    \n","    def forward(self, x, mask=None):\n","        b,t,e = x.size()\n","        if self.positional_encoding:\n","            positions = self.pos[:, :t, :]\n","        else:\n","            positions = self.pos(torch.arange(t, \n","                device=d()))[None, :, :].expand(b, t, e)\n","        x = x + positions\n","        #another dropout layer here?\n","        #pass input batch and mask through layers\n","        for layer in self.layers:\n","            x  = layer(x, mask)\n","        #one last normalization for good measure\n","        z = self.norm(x)\n","        return self.to_scores(z)\n","\n","class DecoderLayer(nn.Module):\n","\n","    def __init__(self, size, n_heads, d_feedforward, dropout,\n","            relative_pos):\n","\n","        super().__init__()\n","        self.self_attn = MultiheadedAttention(size, n_heads,\n","                dropout, relative_pos)\n","        self.feed_forward = PositionwiseFeedForward(size, d_feedforward, dropout)\n","        self.size = size\n","        #normalize over mean/std of embedding dimension\n","        self.norm1 = nn.LayerNorm(size)\n","        self.norm2 = nn.LayerNorm(size)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","\n","    def forward(self, x, mask):\n","        #perform masked attention on input\n","        #masked so queries cannot attend to subsequent keys\n","        #Pass through sublayers of attention and feedforward.\n","        #Apply dropout to sublayer output, add it to input, and norm.\n","        attn = self.self_attn(x, mask)\n","        x = x + self.dropout1(attn)\n","        x = self.norm1(x)\n","\n","        ff = self.feed_forward(x)\n","        x = x + self.dropout2(ff)\n","        x = self.norm2(x)\n","\n","        return x\n","\n","class PositionwiseFeedForward(nn.Module):\n","\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super().__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff)\n","        self.w_2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n","\n","class SequenceEmbedding(nn.Module):\n","    \"\"\"\n","    Standard embedding, scaled by the sqrt of model's hidden state size\n","    \"\"\"\n","    def __init__(self, vocab_size, model_size):\n","        super().__init__()\n","        self.d_model = model_size\n","        self.emb = nn.Embedding(vocab_size, model_size)\n","\n","    def forward(self, x):\n","        return self.emb(x) * math.sqrt(self.d_model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1n5gRbHY1hhT"},"source":["class TransGenerator(nn.Module):\n","    def __init__(self, z_dim, n_tokens, seq_length=None, d_model=64,\n","            n_heads=4, depth=2, d_feedforward=512, dropout=0.1,\n","            positional_encoding=False, relative_pos=True):\n","        \"\"\"\n","        Args:\n","            n_tokens: number of commands/states in encoded musical sequence\n","            seq_length: length of (padded) input/target sequences\n","            d_model: dimensionality of embedded sequences\n","            n_heads: number of attention heads\n","            depth: number of stacked transformer layers\n","            d_feedforward: dimensionality of dense sublayer \n","            dropout: probability of dropout in dropout sublayer\n","            relative_pos: (bool) if True, use relative positional embeddings\n","        \"\"\"\n","        super().__init__()\n","        self.z_dim = z_dim\n","        self.init_mlp = nn.Linear(z_dim, seq_length)\n","        self.norm = nn.LayerNorm(seq_length)\n","        self.relu = nn.ReLU()\n","        self.trans_block = MusicTransformer(n_tokens, seq_length, d_model, n_heads, depth, d_feedforward, dropout, positional_encoding, relative_pos)\n","\n","    def forward(self, noise):\n","        x = self.init_mlp(noise)\n","        x = self.norm(x)\n","        x = self.relu(x).transpose(1, 2)\n","        x = self.trans_block(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ulfZiz-y3Kqi"},"source":["class TransDiscriminator(nn.Module):\n","    def __init__(self, n_tokens, seq_length=None, d_model=64,\n","            n_heads=4, depth=2, d_feedforward=512, dropout=0.1,\n","            positional_encoding=False, relative_pos=True):\n","        \"\"\"\n","        Args:\n","            n_tokens: number of commands/states in encoded musical sequence\n","            seq_length: length of (padded) input/target sequences\n","            d_model: dimensionality of embedded sequences\n","            n_heads: number of attention heads\n","            depth: number of stacked transformer layers\n","            d_feedforward: dimensionality of dense sublayer \n","            dropout: probability of dropout in dropout sublayer\n","            relative_pos: (bool) if True, use relative positional embeddings\n","        \"\"\"\n","        super().__init__()\n","        self.trans_block = MusicTransformer(n_tokens, seq_length, d_model, n_heads, depth, d_feedforward, dropout, positional_encoding, relative_pos, True)\n","        self.clf = nn.Linear(n_tokens, 1)\n","\n","    def forward(self, x):\n","        x = self.trans_block(x)\n","        x = torch.mean(x, dim=1)\n","        x = self.clf(x)\n","\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bAgMkKkB5zjZ"},"source":["# input shape = (N, Seq_len)\n","# label shape = (N, Seq_len)\n","# output shape = (N, Seq_len, N_tokens)\n","# embedded output = (N, Seq_len, d_model)"]},{"cell_type":"code","metadata":{"id":"E9K0l8WFwTxy"},"source":["sampling_rate = 125\n","n_velocity_bins = 32\n","seq_length = 256\n","\n","pipeline = PreprocessingPipeline(input_dir=\"data\", stretch_factors=[0.975, 1, 1.025],\n","        split_size=30, sampling_rate=sampling_rate, n_velocity_bins=n_velocity_bins,\n","        transpositions=range(-2,3), training_val_split=0.9, max_encoded_length=seq_length,\n","                                min_encoded_length=257)\n","pipeline_start = time.time()\n","pipeline.run()\n","runtime = time.time() - pipeline_start\n","print(f\"MIDI pipeline runtime: {runtime / 60 : .1f}m\")\n","\n","today = datetime.date.today().strftime('%m%d%Y')\n","checkpoint = f\"saved_models/tf_{today}\"\n","\n","training_sequences = pipeline.encoded_sequences['training']\n","validation_sequences = pipeline.encoded_sequences['validation'] "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mqHPiy3b3ety"},"source":["def prepare_batches(sequences, batch_size):\n","    \"\"\"\n","    Splits a list of sequences into batches of a fixed size. Each sequence yields an input sequence\n","    and a target sequence, with the latter one time step ahead. For example, the sequence \"to be or not\n","    to be\" gives an input sequence of \"to be or not to b\" and a target sequence of \"o be or not to be.\"\n","    \"\"\"\n","    n_sequences = len(sequences)\n","    for i in range(0, n_sequences, batch_size):\n","        batch = sequences[i:i+batch_size]\n","\t#needs to be in sorted order for packing batches to work\n","        batch = sorted(batch, key = len, reverse=True)\n","        input_sequences, target_sequences = [], []\n","\n","        for sequence in batch:\n","            input_sequences.append(sequence)\n","            target_sequences.append(sequence)\n","\n","        yield input_sequences, target_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5U03E3Z2Ps9P"},"source":["def batch_to_tensors(batch, n_tokens, max_length):\n","    \"\"\"\n","    Make input, input mask, and target tensors for a batch of seqa batch of sequences.\n","    \"\"\"\n","    input_sequences, target_sequences = batch\n","    sequence_lengths = [len(s) for s in input_sequences]\n","    batch_size = len(input_sequences)\n","\n","    x = torch.zeros(batch_size, max_length, dtype=torch.long)\n","    #padding element\n","    y = torch.zeros(batch_size, max_length, dtype=torch.long)\n","\n","\n","    for i, sequence in enumerate(input_sequences):\n","        seq_length = sequence_lengths[i]\n","        #copy over input sequence data with zero-padding\n","        #cast to long to be embedded into model's hidden dimension\n","        x[i, :seq_length] = torch.Tensor(sequence).unsqueeze(0)\n","    \n","    x_mask = (x != 0)\n","    x_mask = x_mask.type(torch.uint8)\n","\n","    for i, sequence in enumerate(target_sequences):\n","        seq_length = sequence_lengths[i]\n","        y[i, :seq_length] = torch.Tensor(sequence).unsqueeze(0)\n","\n","    if torch.cuda.is_available():\n","        return x.cuda(), y.cuda(), x_mask.cuda()\n","    else:\n","        return x, y, x_mask "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7hdrmEWvQbGM"},"source":["max_length = max((len(L) \n","        for L in (training_sequences + validation_sequences))) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vbsBiXNLQn61"},"source":["sampling_rate = 125\n","n_velocity_bins = 32\n","seq_length = 256 \n","n_tokens = 256 + sampling_rate + n_velocity_bins\n","generator = TransGenerator(50, n_tokens, seq_length, \n","            d_model = 64, n_heads = 8, d_feedforward=256, \n","            depth = 4, dropout = 0.0, positional_encoding=True, relative_pos=True)\n","discriminator = TransDiscriminator(n_tokens, seq_length, \n","            d_model = 64, n_heads = 8, d_feedforward=256, \n","            depth = 4, dropout = 0.2, positional_encoding=True, relative_pos=True)\n","generator.to(d()) \n","discriminator.to(d()) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8iT0shI8ShOJ"},"source":["epoch = 20\n","batch_size = 64\n","criterion = nn.BCEWithLogitsLoss()\n","d_lr = 0.0002\n","g_lr = 0.0002\n","gen_opt = torch.optim.Adam(generator.parameters(), lr=g_lr) \n","disc_opt = torch.optim.Adam(discriminator.parameters(), lr=d_lr) \n","PATH = '/content/drive/MyDrive/musictransgan.pt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gw_twNPGQMik"},"source":["G_losses = []\n","D_losses = []\n","disc_real = []\n","disc_fake = []\n","disc_fool = []\n","\n","for e in range(epoch):\n","    training_batches = prepare_batches(training_sequences, batch_size) \n","    for k, batch in enumerate(training_batches):\n","        generator.train()\n","        discriminator.train()\n","\n","        x, y, _ = batch_to_tensors(batch, n_tokens, \n","                        max_length)\n","        \n","        mean_iteration_critic_loss = 0\n","        # Discriminator true outputs\n","        disc_opt.zero_grad()\n","        print(x.shape)\n","        y_disc_real = discriminator(x).view(-1)\n","        disc_loss_real = criterion(y_disc_real, torch.ones_like(y_disc_real))\n","\n","        # Discriminator outputs via Generotor (False)\n","        noise = torch.randn(batch_size, 64, 50) # N, d_model, z_dim\n","        noise = noise.to(d())\n","        x_gen_fake = generator(noise)\n","        y_disc_fake = discriminator(x_gen_fake.detach()).view(-1)\n","        \n","        disc_loss_fake = criterion(y_disc_fake, torch.zeros_like(y_disc_fake))\n","\n","        # Update gradients\n","        disc_loss.backward(retain_graph=True)\n","        # Optim\n","        disc_opt.step()\n","\n","        # Generator\n","        gen_opt.zero_grad()\n","\n","        noise_gen = torch.randn(batch_size, 64, 50) # N, d_model, z_dim\n","        noise_gen = noise_gen.to(d())\n","        x_gen_fool = generator(noise_gen)\n","        y_disc_fool = discriminator(x_gen_fool).view(-1)\n","        loss_gen = criterion(y_disc_fool, torch.ones_like(y_disc_fool))\n","\n","        loss_gen.backward()\n","        # Optim\n","        gen_opt.step()\n","\n","        # Statistics\n","        D_losses.append(disc_loss.item())\n","        G_losses.append(loss_gen.item())\n","        \n","        if k % iter == 0:\n","            print('Epoch: {}, Iter: {}, G_loss: {}, D_loss: {}'.format(e+1, k, loss_gen, loss_disc))\n","\n","    torch.save({\n","        'epoch': e,\n","        'batch_size': batch_size,\n","        'lr': [d_lr, g_lr],\n","        'generator_model_state_dict': generator.state_dict(),\n","        'generator_optimizer_state_dict': gen_opt.state_dict(),\n","        'discriminator_model_state_dict': discriminator.state_dict(),\n","        'discriminator_optimizer_state_dict': disc_opt.state_dict(),\n","        'G_losses': G_losses,\n","        'D_losses': D_losses,\n","    }, PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9l1xxXgVUL1h"},"source":["\"\"\"\n","Generate a MIDI event sequence of a fixed length by randomly sampling from a model's distribution of sequences. Optionally, \"seed\" the sequence with a prime. A well-trained model will create music that responds to the prime and develops upon it.\n","\"\"\"\n","prime_sequence = []\n","temperature = 1\n","#deactivate training mode\n","model.eval()\n","if len(prime_sequence) == 0:\n","    #if no prime is provided, randomly select a starting event\n","    input_sequence = [np.random.randint(model.n_tokens)]\n","else:\n","    input_sequence = prime_sequence.copy()\n","\n","#add singleton dimension for the batch\n","input_tensor = torch.LongTensor(input_sequence).unsqueeze(0).cuda()\n","for i in range(1024):\n","    #select probabilities of *next* token\n","    out = model(input_tensor)[0, -1, :]\n","    #out is a 1d tensor of shape (n_tokens)\n","    probs = F.softmax(out / temperature, dim=0)\n","    #sample prob distribution for next character\n","    c = torch.multinomial(probs,1)\n","    input_tensor = torch.cat([input_tensor[:,1:], c[None]], dim=1)\n","    input_sequence.append(c.item()) "],"execution_count":null,"outputs":[]}]}